{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abcd5a44-2c7c-4be8-ab54-c8b8216521ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "import numpy as np\n",
    "# Import the required modules and classes\n",
    "from sparknlp.base import DocumentAssembler, Pipeline, Finisher\n",
    "\n",
    "from sparknlp.annotator import (\n",
    "    SentenceDetector,\n",
    "    Tokenizer,\n",
    "    Lemmatizer,\n",
    "    SentimentDetector\n",
    ")\n",
    "import pyspark.sql.functions as F\n",
    "# Start Spark Session\n",
    "\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "import afinn\n",
    "from pyspark.sql import types\n",
    "import yahooquery as yq\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import seaborn as sns\n",
    "from pyspark.streaming import dstream,StreamingContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "798235c7-e4f4-4347-8593-2669296d38d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/13 00:44:44 WARN Utils: Your hostname, alber-victus resolves to a loopback address: 127.0.1.1; using 192.168.1.25 instead (on interface wlp4s0)\n",
      "24/05/13 00:44:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/alber/.ivy2/cache\n",
      "The jars for the packages stored in: /home/alber/.ivy2/jars\n",
      "com.johnsnowlabs.nlp#spark-nlp_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-79ca612a-f9bb-41c2-bcb9-70a51d5c0e35;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.johnsnowlabs.nlp#spark-nlp_2.12;5.3.3 in central\n",
      "\tfound com.typesafe#config;1.4.2 in central\n",
      "\tfound org.rocksdb#rocksdbjni;6.29.5 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-s3;1.12.500 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-kms;1.12.500 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-core;1.12.500 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound commons-codec#commons-codec;1.15 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.13 in central\n",
      "\tfound software.amazon.ion#ion-java;1.0.2 in central\n",
      "\tfound joda-time#joda-time;2.8.1 in central\n",
      "\tfound com.amazonaws#jmespath-java;1.12.500 in central\n",
      "\tfound com.github.universal-automata#liblevenshtein;3.0.0 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.0.0-beta-3 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.0.0-beta-3 in central\n",
      "\tfound com.google.code.gson#gson;2.3 in central\n",
      "\tfound it.unimi.dsi#fastutil;7.0.12 in central\n",
      "\tfound org.projectlombok#lombok;1.16.8 in central\n",
      "\tfound com.google.cloud#google-cloud-storage;2.20.1 in central\n",
      "\tfound com.google.guava#guava;31.1-jre in central\n",
      "\tfound com.google.guava#failureaccess;1.0.1 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.18.0 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.3 in central\n",
      "\tfound com.google.http-client#google-http-client;1.43.0 in central\n",
      "\tfound io.opencensus#opencensus-contrib-http-util;0.31.1 in central\n",
      "\tfound com.google.http-client#google-http-client-jackson2;1.43.0 in central\n",
      "\tfound com.google.http-client#google-http-client-gson;1.43.0 in central\n",
      "\tfound com.google.api-client#google-api-client;2.2.0 in central\n",
      "\tfound com.google.oauth-client#google-oauth-client;1.34.1 in central\n",
      "\tfound com.google.http-client#google-http-client-apache-v2;1.43.0 in central\n",
      "\tfound com.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 in central\n",
      "\tfound com.google.code.gson#gson;2.10.1 in central\n",
      "\tfound com.google.cloud#google-cloud-core;2.12.0 in central\n",
      "\tfound io.grpc#grpc-context;1.53.0 in central\n",
      "\tfound com.google.auto.value#auto-value-annotations;1.10.1 in central\n",
      "\tfound com.google.auto.value#auto-value;1.10.1 in central\n",
      "\tfound javax.annotation#javax.annotation-api;1.3.2 in central\n",
      "\tfound com.google.cloud#google-cloud-core-http;2.12.0 in central\n",
      "\tfound com.google.http-client#google-http-client-appengine;1.43.0 in central\n",
      "\tfound com.google.api#gax-httpjson;0.108.2 in central\n",
      "\tfound com.google.cloud#google-cloud-core-grpc;2.12.0 in central\n",
      "\tfound io.grpc#grpc-alts;1.53.0 in central\n",
      "\tfound io.grpc#grpc-grpclb;1.53.0 in central\n",
      "\tfound org.conscrypt#conscrypt-openjdk-uber;2.5.2 in central\n",
      "\tfound io.grpc#grpc-auth;1.53.0 in central\n",
      "\tfound io.grpc#grpc-protobuf;1.53.0 in central\n",
      "\tfound io.grpc#grpc-protobuf-lite;1.53.0 in central\n",
      "\tfound io.grpc#grpc-core;1.53.0 in central\n",
      "\tfound com.google.api#gax;2.23.2 in central\n",
      "\tfound com.google.api#gax-grpc;2.23.2 in central\n",
      "\tfound com.google.auth#google-auth-library-credentials;1.16.0 in central\n",
      "\tfound com.google.auth#google-auth-library-oauth2-http;1.16.0 in central\n",
      "\tfound com.google.api#api-common;2.6.2 in central\n",
      "\tfound io.opencensus#opencensus-api;0.31.1 in central\n",
      "\tfound com.google.api.grpc#proto-google-iam-v1;1.9.2 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.21.12 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.21.12 in central\n",
      "\tfound com.google.api.grpc#proto-google-common-protos;2.14.2 in central\n",
      "\tfound org.threeten#threetenbp;1.6.5 in central\n",
      "\tfound com.google.api.grpc#proto-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.api.grpc#grpc-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.api.grpc#gapic-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound io.grpc#grpc-api;1.53.0 in central\n",
      "\tfound io.grpc#grpc-stub;1.53.0 in central\n",
      "\tfound org.checkerframework#checker-qual;3.31.0 in central\n",
      "\tfound io.perfmark#perfmark-api;0.26.0 in central\n",
      "\tfound com.google.android#annotations;4.1.1.4 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.22 in central\n",
      "\tfound io.opencensus#opencensus-proto;0.2.0 in central\n",
      "\tfound io.grpc#grpc-services;1.53.0 in central\n",
      "\tfound com.google.re2j#re2j;1.6 in central\n",
      "\tfound io.grpc#grpc-netty-shaded;1.53.0 in central\n",
      "\tfound io.grpc#grpc-googleapis;1.53.0 in central\n",
      "\tfound io.grpc#grpc-xds;1.53.0 in central\n",
      "\tfound com.navigamez#greex;1.0 in central\n",
      "\tfound dk.brics.automaton#automaton;1.11-8 in central\n",
      "\tfound com.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 in central\n",
      "\tfound com.microsoft.onnxruntime#onnxruntime;1.17.0 in central\n",
      ":: resolution report :: resolve 771ms :: artifacts dl 19ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-core;1.12.500 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-kms;1.12.500 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-s3;1.12.500 from central in [default]\n",
      "\tcom.amazonaws#jmespath-java;1.12.500 from central in [default]\n",
      "\tcom.github.universal-automata#liblevenshtein;3.0.0 from central in [default]\n",
      "\tcom.google.android#annotations;4.1.1.4 from central in [default]\n",
      "\tcom.google.api#api-common;2.6.2 from central in [default]\n",
      "\tcom.google.api#gax;2.23.2 from central in [default]\n",
      "\tcom.google.api#gax-grpc;2.23.2 from central in [default]\n",
      "\tcom.google.api#gax-httpjson;0.108.2 from central in [default]\n",
      "\tcom.google.api-client#google-api-client;2.2.0 from central in [default]\n",
      "\tcom.google.api.grpc#gapic-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#grpc-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-common-protos;2.14.2 from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-iam-v1;1.9.2 from central in [default]\n",
      "\tcom.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-credentials;1.16.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-oauth2-http;1.16.0 from central in [default]\n",
      "\tcom.google.auto.value#auto-value;1.10.1 from central in [default]\n",
      "\tcom.google.auto.value#auto-value-annotations;1.10.1 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-grpc;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-http;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-storage;2.20.1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.10.1 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.18.0 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0.1 from central in [default]\n",
      "\tcom.google.guava#guava;31.1-jre from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.http-client#google-http-client;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-apache-v2;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-appengine;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-gson;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-jackson2;1.43.0 from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.3 from central in [default]\n",
      "\tcom.google.oauth-client#google-oauth-client;1.34.1 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.21.12 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.21.12 from central in [default]\n",
      "\tcom.google.re2j#re2j;1.6 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#spark-nlp_2.12;5.3.3 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 from central in [default]\n",
      "\tcom.microsoft.onnxruntime#onnxruntime;1.17.0 from central in [default]\n",
      "\tcom.navigamez#greex;1.0 from central in [default]\n",
      "\tcom.typesafe#config;1.4.2 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.15 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tdk.brics.automaton#automaton;1.11-8 from central in [default]\n",
      "\tio.grpc#grpc-alts;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-api;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-auth;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-context;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-core;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-googleapis;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-grpclb;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-netty-shaded;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf-lite;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-services;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-stub;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-xds;1.53.0 from central in [default]\n",
      "\tio.opencensus#opencensus-api;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-contrib-http-util;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-proto;0.2.0 from central in [default]\n",
      "\tio.perfmark#perfmark-api;0.26.0 from central in [default]\n",
      "\tit.unimi.dsi#fastutil;7.0.12 from central in [default]\n",
      "\tjavax.annotation#javax.annotation-api;1.3.2 from central in [default]\n",
      "\tjoda-time#joda-time;2.8.1 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.13 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.31.0 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.22 from central in [default]\n",
      "\torg.conscrypt#conscrypt-openjdk-uber;2.5.2 from central in [default]\n",
      "\torg.projectlombok#lombok;1.16.8 from central in [default]\n",
      "\torg.rocksdb#rocksdbjni;6.29.5 from central in [default]\n",
      "\torg.threeten#threetenbp;1.6.5 from central in [default]\n",
      "\tsoftware.amazon.ion#ion-java;1.0.2 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcommons-logging#commons-logging;1.2 by [commons-logging#commons-logging;1.1.3] in [default]\n",
      "\tcommons-codec#commons-codec;1.11 by [commons-codec#commons-codec;1.15] in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.0.0-beta-3 by [com.google.protobuf#protobuf-java-util;3.21.12] in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.0.0-beta-3 by [com.google.protobuf#protobuf-java;3.21.12] in [default]\n",
      "\tcom.google.code.gson#gson;2.3 by [com.google.code.gson#gson;2.10.1] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   83  |   0   |   0   |   5   ||   78  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-79ca612a-f9bb-41c2-bcb9-70a51d5c0e35\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 78 already retrieved (0kB/11ms)\n",
      "24/05/13 00:44:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# spark = SparkSession.builder.appName(\"sentiment\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f427d9e-9a47-429d-89dc-0781d5d6f1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! wget -N https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/lemma-corpus-small/lemmas_small.txt -P /tmp\n",
    "# ! wget -N https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/sentiment-corpus/default-sentiment-dict.txt -P /tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71f23d7c-82d6-48a9-8472-b2fb7d983837",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_scores(column='article',stock='Tesla'):\n",
    "\n",
    "\n",
    "    data = spark.read.option(\"header\",\"true\").json(\"../data/json/\"+stock)\n",
    "    tickers = {\"NVIDIA\":\"NVDA\",\n",
    "              \"Bitcoin\":\"BTC-USD\",\n",
    "              \"Apple\":\"AAPL\",\n",
    "              \"Tesla\":\"TSLA\"}\n",
    "    \n",
    "    # Step 1: Transforms raw texts to `document` annotation\n",
    "    document_assembler = (\n",
    "        DocumentAssembler() \\\n",
    "        .setInputCol(column) \\\n",
    "        .setOutputCol(\"document\")\n",
    "    )\n",
    "\n",
    "    schema = StructType([\n",
    "    StructField(\"article\",StringType(),True),\n",
    "    StructField(\"description\",StringType(),True),\n",
    "    StructField(\"published date\",StringType(),True),\n",
    "    StructField(\"publisher_url\",StringType(),True),\n",
    "    StructField(\"title\",StringType(),True),\n",
    "    StructField(\"url\",StringType(),True)\n",
    "    ])\n",
    "\n",
    "    \n",
    "    \n",
    "    # Step 2: Sentence Detection\n",
    "    sentence_detector = SentenceDetector().setInputCols([\"document\"]).setOutputCol(\"sentence\")\n",
    "    \n",
    "    # Step 3: Tokenization\n",
    "    tokenizer = Tokenizer().setInputCols([\"sentence\"]).setOutputCol(\"token\")\n",
    "    \n",
    "    # Step 4: Lemmatization\n",
    "    lemmatizer= Lemmatizer().setInputCols(\"token\").setOutputCol(\"lemma\") \\\n",
    "                            .setDictionary(\"lemmas_small.txt\", key_delimiter=\"->\", value_delimiter=\"\\t\")\n",
    "    \n",
    "    # Step 5: Sentiment Detection\n",
    "    sentiment_detector= (\n",
    "        SentimentDetector() \\\n",
    "        .setInputCols([\"lemma\", \"sentence\"]) \\\n",
    "        .setOutputCol(\"sentiment_score\") \\\n",
    "        .setDictionary(\"default-sentiment-dict.txt\", \",\")\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Step 6: Finisher\n",
    "    finisher= (\n",
    "        Finisher() \\\n",
    "        .setInputCols([\"sentiment_score\"]).setOutputCols(\"sentiment\")\n",
    "    )\n",
    "    \n",
    "    # Define the pipeline\n",
    "    pipeline = Pipeline(\n",
    "        stages=[\n",
    "            document_assembler,\n",
    "            sentence_detector, \n",
    "            tokenizer, \n",
    "            lemmatizer, \n",
    "            sentiment_detector, \n",
    "            finisher\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    def compute_with_afinn(text):\n",
    "    \n",
    "        return afinn.Afinn().score(text)\n",
    "    \n",
    "    compute_sentiment_score_udf = F.udf(compute_with_afinn, types.FloatType())\n",
    "\n",
    "    # data = data.withColumnRe\n",
    "\n",
    "    # scores\n",
    "    result = pipeline.fit(data).transform(data)\n",
    "    \n",
    "    result = result.withColumn(\"afinn_sentiment\",compute_sentiment_score_udf(F.col('article')))\n",
    "    \n",
    "    result = result.withColumn(\"pnn_sentiment\",\n",
    "                             F.when(F.array_contains(F.col(\"sentiment\"), \"positive\"), 1)\n",
    "                              .when(F.array_contains(F.col(\"sentiment\"), \"negative\"), -1)\n",
    "                              .otherwise(0))\n",
    "    result = result.withColumnRenamed(\"published date\",\"published_date\")\n",
    "    result = result.withColumn(\"published_date\", F.regexp_extract(F.col(\"published_date\"), r\"^.*,\\s*(.+)\\s\\d+:\\d+:\\d+\", 1))\n",
    "\n",
    "    func = F.udf(lambda x: dt.datetime.strptime(x, '%d %b %Y'), DateType())\n",
    "    \n",
    "    # Apply UDF to the column\n",
    "    result = result.withColumn('published_date', func(result['published_date']))\n",
    "\n",
    "    result = result.select('published_date','afinn_sentiment','pnn_sentiment')\n",
    "\n",
    "\n",
    "    result = result.groupBy(\"published_date\").agg(F.mean(\"afinn_sentiment\").alias(\"afinn_sentiment_mean\"),\\\n",
    "                                                  F.mean(\"pnn_sentiment\").alias(\"pnn_sentiment\"))\n",
    "\n",
    "    \n",
    "    feature_columns = [\"afinn_sentiment_mean\"]\n",
    "\n",
    "    # scale\n",
    "    assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"afinn_sentiment_scaled\")\n",
    "\n",
    "    # Scale the features using MinMaxScaler\n",
    "    scaler = StandardScaler(inputCol=\"afinn_sentiment_scaled\", outputCol=\"afinn_sentiment\")\n",
    "    \n",
    "    # Create a pipeline to execute the assembler and scaler\n",
    "    pipeline = Pipeline(stages=[assembler, scaler])\n",
    "    \n",
    "    # Fit and transform the pipeline\n",
    "    result = pipeline.fit(result).transform(result)\n",
    "\n",
    "\n",
    "    firstelement = F.udf(lambda v:float(v[0]),FloatType())\n",
    "    result = result.withColumn(\"afinn_sentiment\", firstelement(\"afinn_sentiment\"))\n",
    "    \n",
    "    result = result.withColumnRenamed(\"published_date\",\"date\")\n",
    "    \n",
    "    result = result.select('date','afinn_sentiment','pnn_sentiment')\n",
    "\n",
    "\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def prepare_price(stock,start_date=None,end_date=None):\n",
    "\n",
    "    price = yq.Ticker(stock).history(start=start_date,end=end_date).reset_index()\n",
    "    # return price\n",
    "\n",
    "    price_df = spark.createDataFrame(price)\n",
    "\n",
    "\n",
    "    # only adj close and volume\n",
    "    price_df = price_df.select(\"date\",\"volume\",\"adjclose\")\n",
    "\n",
    "    # scale\n",
    "    \n",
    "    def compute_percent_change(current_price, previous_price):\n",
    "        if current_price is not None and previous_price is not None:\n",
    "            return ((current_price - previous_price) / previous_price) * 100\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    percent_change_udf = F.udf(compute_percent_change, FloatType())\n",
    "    \n",
    "    # Calculate percent change\n",
    "    window_spec = Window.orderBy(\"date\")\n",
    "    \n",
    "    price_df = price_df.withColumn(\"prev_price\", F.lag(\"adjclose\", 1).over(window_spec))\n",
    "    price_df = price_df.withColumn(\"price_percent_change\", percent_change_udf(F.col(\"adjclose\"), F.col(\"prev_price\")))\n",
    "    \n",
    "    \n",
    "    price_df = price_df.withColumn(\"prev_volume\", F.lag(\"volume\", 1).over(window_spec))\n",
    "    price_df = price_df.withColumn(\"volume_percent_change\", percent_change_udf(F.col(\"volume\"), F.col(\"prev_volume\")))\n",
    "    \n",
    "    \n",
    "    price_df = price_df.select('date','price_percent_change','volume_percent_change')   \n",
    "    price_df = price_df.withColumn(\"next_day_price_percent_change_shifted\", F.lead(\"price_percent_change\", 1).over(window_spec))\n",
    "\n",
    "    return price_df\n",
    "\n",
    "\n",
    "def prepare_mix_data(scores,stock='NVDA'):\n",
    "\n",
    "    min_date = scores.agg(F.min(\"date\")).collect()[0][0].strftime('%Y-%m-%d')\n",
    "    max_date = scores.agg(F.max(\"date\")).collect()[0][0].strftime('%Y-%m-%d')\n",
    "\n",
    "    price_df = prepare_price(stock,min_date,max_date)\n",
    "\n",
    "    df = scores.join(price_df, on=\"date\", how=\"right\")\n",
    "    df = df.dropna()\n",
    "\n",
    "    return df\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "349a9106-20ee-425e-90ba-09a472db7e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alber/.local/lib/python3.11/site-packages/yahooquery/utils/__init__.py:1470: FutureWarning: 'S' is deprecated and will be removed in a future version. Please use 's' instead of 'S'.\n",
      "  has_live_indice = index_utc[-1] >= last_trade - pd.Timedelta(2, \"S\")\n"
     ]
    }
   ],
   "source": [
    "scores = get_scores()\n",
    "\n",
    "df = prepare_mix_data(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54301738-9b69-4d27-aa60-b91a0646ca59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/13 00:58:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 00:58:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 00:58:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 00:58:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 00:58:55 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 00:58:55 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 00:58:55 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 00:58:55 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 00:58:55 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 00:58:55 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 00:58:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 00:58:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+------------------+--------------------+---------------------+-------------------------------------+\n",
      "|      date|afinn_sentiment|     pnn_sentiment|price_percent_change|volume_percent_change|next_day_price_percent_change_shifted|\n",
      "+----------+---------------+------------------+--------------------+---------------------+-------------------------------------+\n",
      "|2017-01-04|     0.83295304|0.9354838709677419|           2.3330994|           -20.158243|                           -2.5385501|\n",
      "|2017-01-05|      1.7214363|0.8666666666666667|          -2.5385501|           -17.921982|                            1.3367301|\n",
      "|2017-01-06|       1.813135|               1.0|           1.3367301|            -16.40157|                            4.0543356|\n",
      "|2017-01-09|      1.3718747|               1.0|           4.0543356|            11.349738|                           -0.7550339|\n",
      "|2017-01-10|      2.9906306|0.7777777777777778|          -0.7550339|           -3.8557246|                           -1.2303915|\n",
      "|2017-01-11|      1.5591398|0.9166666666666666|          -1.2303915|            -40.32784|                           -1.6356237|\n",
      "|2017-01-12|     0.90772533|0.8518518518518519|          -1.6356237|            19.014427|                         -0.009661256|\n",
      "|2017-01-13|      0.8534239|               1.0|        -0.009661256|           -26.820925|                           -2.2430725|\n",
      "|2017-01-17|     0.45224172|               1.0|          -2.2430725|            26.821022|                               1.8198|\n",
      "|2017-01-18|     0.50877196|               0.9|              1.8198|            12.712792|                            2.1466987|\n",
      "|2017-01-19|    -0.75026274|0.7857142857142857|           2.1466987|           -1.4596041|                           -1.0935787|\n",
      "|2017-01-20|     -0.6564799|               1.0|          -1.0935787|           -23.702068|                            1.0383375|\n",
      "|2017-01-23|      1.0292957|0.9259259259259259|           1.0383375|           -24.819927|                              2.13155|\n",
      "|2017-01-24|      1.8263608|0.5384615384615384|             2.13155|            15.719754|                           0.42857733|\n",
      "|2017-01-25|      1.6710398|               1.0|          0.42857733|            22.125349|                            1.7255434|\n",
      "|2017-01-26|     0.20514998|               1.0|           1.7255434|            -12.23506|                            1.9334446|\n",
      "|2017-01-27|      1.5317864|               0.8|           1.9334446|            2.8761988|                           -1.5657179|\n",
      "|2017-01-30|       2.242973|               0.9|          -1.5657179|             5.086484|                          -0.76350164|\n",
      "|2017-01-31|      1.8912873|0.5238095238095238|         -0.76350164|             -26.8643|                             4.368945|\n",
      "|2017-02-01|      0.7629361|0.8918918918918919|            4.368945|            62.644863|                            1.2637053|\n",
      "+----------+---------------+------------------+--------------------+---------------------+-------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/13 00:58:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 00:58:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec94c61-7ba2-44bf-ae07-b8ce412d1e13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "574f1dae-c3e6-42a9-bec0-29b8126f6d95",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3b4514-c8cd-4715-8bce-8d0afd7bde04",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max = MinMaxScaler((-1,1))\n",
    "std_scaler = StandardScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea3349f-d11b-46af-b8dc-25cd13ca67c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = df.copy()\n",
    "model_df['date'] = model_df['published date'].apply(lambda x: dt.datetime.strptime(x, \"%d %b %Y\").strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "model_df = model_df[['date','sentiment','afinn_sentiment']]\n",
    "# date_obj = \n",
    "\n",
    "# Convert to the desired format\n",
    "# formatted_date = \n",
    "# pd.to_datetime(model_df['published date'],format=\"%Y-%m-%d\")\n",
    "\n",
    "min_max_results = min_max.fit_transform(model_df['afinn_sentiment'].values.reshape(-1,1))\n",
    "std_results = std_scaler.fit_transform(model_df['afinn_sentiment'].values.reshape(-1,1))\n",
    "\n",
    "model_df['min_max_afinn'] = min_max_results\n",
    "model_df['std_afinn'] = std_results\n",
    "model_df['div_to_max'] = model_df['afinn_sentiment']/model_df['afinn_sentiment'].max()\n",
    "model_df = model_df.groupby(\"date\").sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff0a374-4004-46d3-ad53-f80fce970a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adce38a-a057-40fa-8153-fd51244364c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df['date'] = pd.to_datetime(model_df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89aece17-7c07-4997-a809-045ead3d461c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df.date.max(),model_df.date.min()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cb863083-2204-4fb0-9fd6-1401c508088e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alber/.local/lib/python3.11/site-packages/yahooquery/utils/__init__.py:1470: FutureWarning: 'S' is deprecated and will be removed in a future version. Please use 's' instead of 'S'.\n",
      "  has_live_indice = index_utc[-1] >= last_trade - pd.Timedelta(2, \"S\")\n",
      "/home/alber/.local/lib/python3.11/site-packages/yahooquery/ticker.py:1333: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"dividends\"].fillna(0, inplace=True)\n",
      "/home/alber/.local/lib/python3.11/site-packages/yahooquery/ticker.py:1335: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"splits\"].fillna(0, inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>adjclose</th>\n",
       "      <th>dividends</th>\n",
       "      <th>splits</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>symbol</th>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"11\" valign=\"top\">NVDA</th>\n",
       "      <th>2017-01-03</th>\n",
       "      <td>26.100000</td>\n",
       "      <td>26.592501</td>\n",
       "      <td>24.844999</td>\n",
       "      <td>25.502501</td>\n",
       "      <td>150199600</td>\n",
       "      <td>25.134892</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-04</th>\n",
       "      <td>25.850000</td>\n",
       "      <td>26.375000</td>\n",
       "      <td>25.382500</td>\n",
       "      <td>26.097500</td>\n",
       "      <td>119922000</td>\n",
       "      <td>25.721317</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-05</th>\n",
       "      <td>26.132500</td>\n",
       "      <td>26.455000</td>\n",
       "      <td>25.262501</td>\n",
       "      <td>25.434999</td>\n",
       "      <td>98429600</td>\n",
       "      <td>25.068356</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-06</th>\n",
       "      <td>25.712500</td>\n",
       "      <td>26.062500</td>\n",
       "      <td>25.299999</td>\n",
       "      <td>25.775000</td>\n",
       "      <td>82285600</td>\n",
       "      <td>25.403461</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-09</th>\n",
       "      <td>25.875000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>25.875000</td>\n",
       "      <td>26.820000</td>\n",
       "      <td>91624800</td>\n",
       "      <td>26.433397</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-06</th>\n",
       "      <td>893.900024</td>\n",
       "      <td>922.200012</td>\n",
       "      <td>890.549988</td>\n",
       "      <td>921.400024</td>\n",
       "      <td>37620300</td>\n",
       "      <td>921.400024</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-07</th>\n",
       "      <td>910.979980</td>\n",
       "      <td>917.809998</td>\n",
       "      <td>823.250000</td>\n",
       "      <td>905.539978</td>\n",
       "      <td>43734200</td>\n",
       "      <td>905.539978</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-08</th>\n",
       "      <td>894.830017</td>\n",
       "      <td>911.940002</td>\n",
       "      <td>894.200012</td>\n",
       "      <td>904.119995</td>\n",
       "      <td>32572100</td>\n",
       "      <td>904.119995</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-09</th>\n",
       "      <td>905.289978</td>\n",
       "      <td>910.719971</td>\n",
       "      <td>882.309998</td>\n",
       "      <td>887.469971</td>\n",
       "      <td>37801300</td>\n",
       "      <td>887.469971</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-10</th>\n",
       "      <td>903.049988</td>\n",
       "      <td>914.010010</td>\n",
       "      <td>892.270020</td>\n",
       "      <td>898.780029</td>\n",
       "      <td>33455700</td>\n",
       "      <td>898.780029</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1851 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         open        high         low       close     volume  \\\n",
       "symbol date                                                                    \n",
       "NVDA   2017-01-03   26.100000   26.592501   24.844999   25.502501  150199600   \n",
       "       2017-01-04   25.850000   26.375000   25.382500   26.097500  119922000   \n",
       "       2017-01-05   26.132500   26.455000   25.262501   25.434999   98429600   \n",
       "       2017-01-06   25.712500   26.062500   25.299999   25.775000   82285600   \n",
       "       2017-01-09   25.875000   27.000000   25.875000   26.820000   91624800   \n",
       "...                       ...         ...         ...         ...        ...   \n",
       "       2024-05-06  893.900024  922.200012  890.549988  921.400024   37620300   \n",
       "       2024-05-07  910.979980  917.809998  823.250000  905.539978   43734200   \n",
       "       2024-05-08  894.830017  911.940002  894.200012  904.119995   32572100   \n",
       "       2024-05-09  905.289978  910.719971  882.309998  887.469971   37801300   \n",
       "       2024-05-10  903.049988  914.010010  892.270020  898.780029   33455700   \n",
       "\n",
       "                     adjclose  dividends  splits  \n",
       "symbol date                                       \n",
       "NVDA   2017-01-03   25.134892        0.0     0.0  \n",
       "       2017-01-04   25.721317        0.0     0.0  \n",
       "       2017-01-05   25.068356        0.0     0.0  \n",
       "       2017-01-06   25.403461        0.0     0.0  \n",
       "       2017-01-09   26.433397        0.0     0.0  \n",
       "...                       ...        ...     ...  \n",
       "       2024-05-06  921.400024        0.0     0.0  \n",
       "       2024-05-07  905.539978        0.0     0.0  \n",
       "       2024-05-08  904.119995        0.0     0.0  \n",
       "       2024-05-09  887.469971        0.0     0.0  \n",
       "       2024-05-10  898.780029        0.0     0.0  \n",
       "\n",
       "[1851 rows x 8 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price = yq.Ticker(\"NVDA\").history(start='2017-01-02')\n",
    "price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0884aa-a07f-4306-b2d0-1a68920dfb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_df = price.reset_index()[['date','adjclose']]\n",
    "price_df['p_return'] = price_df['adjclose'].pct_change(1)\n",
    "price_df = price_df[['date','p_return']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a236897f-bbd9-4ae1-a023-f754e8afee50",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_df['date'] = pd.to_datetime(price_df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96873e6-843e-46ed-bd2b-6106864fcde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = model_df.merge(price_df,on='date',how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1893405-1681-4501-91cc-f93516b14ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b0aee1-0a96-48cd-bd8b-500c599b2029",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb449ea-1af9-4f40-990f-1ec1623f508c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "plt.plot(final_df['date'],final_df['p_return'])\n",
    "plt.plot(final_df['date'],final_df['std_afinn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d066a2-47d6-4e23-bb52-281a317985a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = final_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa1319a-e6fa-408e-918a-c9ee9bc38473",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "sns.heatmap(corr,annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb73097-0ac5-4ca0-8aea-db4ed6be06f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_df = final_df.copy()\n",
    "reg_df = final_df[['sentiment','std_afinn','p_return']]\n",
    "reg_df['next_day_return'] = reg_df['p_return'].shift(-1)\n",
    "reg_df['label'] = np.where(reg_df['next_day_return']<=0,0,1)\n",
    "reg_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acd318b-aab4-476c-b44d-c020adb32bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR,SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb8dd7d-90a5-446a-b931-554937d65c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "X,y = reg_df.iloc[:,:3].values,reg_df.iloc[:,3]\n",
    "\n",
    "n = len(X)\n",
    "train_size = int(0.8*n)\n",
    "\n",
    "X_train,X_test = X[:train_size],X[train_size:]\n",
    "y_train,y_test = y[:train_size],y[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e66c78-f0fc-4e56-8127-3d4d1f8a552a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6965ef27-23c4-4ded-911f-2ee50a4cce98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24affac2-a8ae-4a94-86d0-32e52262eaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbaf97b-dc61-46ff-8b6f-4bc776ac12ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29102434-b9ee-4a6c-b44e-6a34795cbfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51515da4-90f0-4a8b-99ba-9955743d165a",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6def36-0ed2-4328-806d-8ca700e4c58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = svm.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c8c724-ec3a-439c-ba86-0aeb58fa5b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc94c1b3-5e13-42a4-95c7-cdd9ebc62a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5ecae2-800f-41f3-975b-ed9c5e30632a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4225aa9-841b-4310-a274-721d710c5f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = reg_df.iloc[:,:3].values,reg_df.iloc[:,4]\n",
    "\n",
    "n = len(X)\n",
    "train_size = int(0.8*n)\n",
    "\n",
    "X_train,X_test = X[:train_size],X[train_size:]\n",
    "y_train,y_test = y[:train_size],y[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f69c9a-0e72-4a09-b62d-2810ba030413",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression()\n",
    "log_reg = log_reg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c219e5-5001-4cab-8960-f1e8fc31440c",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.score(X_train,y_train),log_reg.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5057302-b1f1-423c-a855-1733266a3f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "svc = svc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fec76a-f022-4cfa-9b9a-f54c4d715304",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc.score(X_train,y_train),svc.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcb7497-25d3-4300-8f39-410c3f7de848",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:============>                                        (331 + 20) / 1411]\r"
     ]
    }
   ],
   "source": [
    "forest = RandomForestClassifier(n_estimators=66,max_depth=5)\n",
    "forest = forest.fit(X_train,y_train)\n",
    "forest.score(X_train,y_train),forest.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe971140-a495-4c44-980f-46812b109776",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=7)\n",
    "knn = knn.fit(X_train,y_train)\n",
    "knn.score(X_train,y_train),knn.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34a6745-2cbb-405d-87a0-d60f60ce8ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9106811-5c05-4e5d-a231-7a727b6969b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_df.value_counts(\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c417609e-94b6-4666-b869-fa2ebfda6b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(max_depth=5)\n",
    "xgb = xgb.fit(X_train,y_train.values.reshape(-1,1))\n",
    "xgb.score(X_train,y_train),xgb.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8be005-3b61-4061-87af-d7504c6f9d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f5b527-bed9-4660-9107-73139f5e51e1",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f034b3bd-1870-4ba3-a493-b07273858e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-13 00:59:30.067690: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-13 00:59:30.091505: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-13 00:59:30.203080: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-13 00:59:30.203146: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-13 00:59:30.203924: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-13 00:59:30.264426: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-13 00:59:30.265693: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-13 00:59:31.040261: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from elephas import ml,ml_model,mllib,enums,parameter,spark_model,utils,worker\n",
    "from elephas.spark_model import SparkModel\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f38fc21-4a4e-4abb-8811-b19184b112b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/13 00:59:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 00:59:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 00:59:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 00:59:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 00:59:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 00:59:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 00:59:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 00:59:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 00:59:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 00:59:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 00:59:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 00:59:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 00:59:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 00:59:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['date',\n",
       " 'afinn_sentiment',\n",
       " 'pnn_sentiment',\n",
       " 'price_percent_change',\n",
       " 'volume_percent_change',\n",
       " 'next_day_price_percent_change_shifted']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = df.count()\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "884a4b7e-6af9-443d-b68a-21416a517532",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/13 01:04:12 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:12 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:12 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:12 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:15 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:15 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:18 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:18 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:18 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:18 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:18 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:18 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:21 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:21 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:21 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:21 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:21 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:21 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:21 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:21 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/13 01:04:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "train_size = int(n*.8)\n",
    "\n",
    "train_data = df.limit(train_size)\n",
    "test_data = df.subtract(train_data)\n",
    "\n",
    "X_train = train_data.select('afinn_sentiment', 'pnn_sentiment', 'price_percent_change','volume_percent_change')\n",
    "y_train = train_data.select(\"next_day_price_percent_change_shifted\")\n",
    "\n",
    "X_train = np.array(X_train.rdd.map(lambda x: [x.afinn_sentiment,x.pnn_sentiment,x.price_percent_change,x.volume_percent_change]).collect())\n",
    "y_train = np.array(y_train.rdd.map(lambda x: [x.next_day_price_percent_change_shifted]).collect())\n",
    "\n",
    "\n",
    "X_test = test_data.select('afinn_sentiment', 'pnn_sentiment', 'price_percent_change','volume_percent_change')\n",
    "y_test = test_data.select(\"next_day_price_percent_change_shifted\")\n",
    "\n",
    "X_test = np.array(X_test.rdd.map(lambda x: [x.afinn_sentiment,x.pnn_sentiment,x.price_percent_change,x.volume_percent_change]).collect())\n",
    "y_test = np.array(y_test.rdd.map(lambda x: [x.next_day_price_percent_change_shifted]).collect())\n",
    "\n",
    "\n",
    "\n",
    "# outdata.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "20bc3601-22c2-409f-81b8-e1b690f3b9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-13 01:04:35.095180: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-13 01:04:35.161350: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2211] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "num_features = 3\n",
    "input_shape = 30\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(150, activation='tanh', return_sequences=True, input_shape=(input_shape, num_features)))\n",
    "model.add(LSTM(64, activation='relu'))\n",
    "model.add(Dense(64))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "54887ce9-6870-41c1-9b57-ff52efb058e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "# sgd = SGD(lr=0.1)\n",
    "# model.compile(sgd, loss='categorical_crossentropy', ['acc'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "42a92db1-ac23-48ef-ad1d-b7fde7975ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 30, 150)           92400     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                55040     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 151665 (592.44 KB)\n",
      "Trainable params: 151665 (592.44 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6a8eaddb-a575-4d74-aa36-3269effe6db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install gunicorn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "148c3510-7ae7-4efc-ad30-0cf807937a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from elephas.utils.rdd_utils import to_simple_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d2d9c6b6-23d7-4bfd-8523-434a84263c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Fit model\n",
      " * Serving Flask app 'elephas.parameter.server'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on http://127.0.1.1:4000\n",
      "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Initialize workers\n",
      ">>> Distribute load\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/alber/.local/lib/python3.11/site-packages/pyspark/serializers.py\", line 459, in dumps\n",
      "    return cloudpickle.dumps(obj, pickle_protocol)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/alber/.local/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"/home/alber/.local/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 632, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: cannot pickle 'weakref.ReferenceType' object\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not serialize object: TypeError: cannot pickle 'weakref.ReferenceType' object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pyspark/serializers.py:459\u001b[0m, in \u001b[0;36mCloudPickleSerializer.dumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 459\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcloudpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mPickleError:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py:73\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, protocol, buffer_callback)\u001b[0m\n\u001b[1;32m     70\u001b[0m cp \u001b[38;5;241m=\u001b[39m CloudPickler(\n\u001b[1;32m     71\u001b[0m     file, protocol\u001b[38;5;241m=\u001b[39mprotocol, buffer_callback\u001b[38;5;241m=\u001b[39mbuffer_callback\n\u001b[1;32m     72\u001b[0m )\n\u001b[0;32m---> 73\u001b[0m \u001b[43mcp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m file\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py:632\u001b[0m, in \u001b[0;36mCloudPickler.dump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 632\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot pickle 'weakref.ReferenceType' object",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m simple_rdd \u001b[38;5;241m=\u001b[39m to_simple_rdd(sc, X_train,y_train)\n\u001b[1;32m      6\u001b[0m spark_model \u001b[38;5;241m=\u001b[39m SparkModel(model, frequency\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124masynchronous\u001b[39m\u001b[38;5;124m'\u001b[39m,batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mspark_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43msimple_rdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/elephas/spark_model.py:188\u001b[0m, in \u001b[0;36mSparkModel.fit\u001b[0;34m(self, rdd, **kwargs)\u001b[0m\n\u001b[1;32m    185\u001b[0m     rdd \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mrepartition(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_workers)\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;129;01min\u001b[39;00m [mode \u001b[38;5;28;01mfor\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m Mode]:\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChoose from one of the modes: asynchronous, synchronous or hogwild\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/elephas/spark_model.py:216\u001b[0m, in \u001b[0;36mSparkModel._fit\u001b[0;34m(self, rdd, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m worker \u001b[38;5;241m=\u001b[39m AsynchronousSparkWorker(\n\u001b[1;32m    214\u001b[0m     model_json, parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient, train_config, freq, optimizer, loss, metrics, custom)\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>>> Distribute load\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 216\u001b[0m \u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>>> Async training complete.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    218\u001b[0m new_parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mget_parameters()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pyspark/rdd.py:1833\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[1;32m   1832\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1833\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonRDD\u001b[38;5;241m.\u001b[39mcollectAndServe(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241m.\u001b[39mrdd())\n\u001b[1;32m   1834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pyspark/rdd.py:5470\u001b[0m, in \u001b[0;36mPipelinedRDD._jrdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5467\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5468\u001b[0m     profiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 5470\u001b[0m wrapped_func \u001b[38;5;241m=\u001b[39m \u001b[43m_wrap_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5471\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prev_jrdd_deserializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd_deserializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofiler\u001b[49m\n\u001b[1;32m   5472\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5474\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   5475\u001b[0m python_rdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonRDD(\n\u001b[1;32m   5476\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prev_jrdd\u001b[38;5;241m.\u001b[39mrdd(), wrapped_func, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreservesPartitioning, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_barrier\n\u001b[1;32m   5477\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pyspark/rdd.py:5268\u001b[0m, in \u001b[0;36m_wrap_function\u001b[0;34m(sc, func, deserializer, serializer, profiler)\u001b[0m\n\u001b[1;32m   5266\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m serializer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserializer should not be empty\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5267\u001b[0m command \u001b[38;5;241m=\u001b[39m (func, profiler, deserializer, serializer)\n\u001b[0;32m-> 5268\u001b[0m pickled_command, broadcast_vars, env, includes \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_for_python_RDD\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5269\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   5270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSimplePythonFunction(\n\u001b[1;32m   5271\u001b[0m     \u001b[38;5;28mbytearray\u001b[39m(pickled_command),\n\u001b[1;32m   5272\u001b[0m     env,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5277\u001b[0m     sc\u001b[38;5;241m.\u001b[39m_javaAccumulator,\n\u001b[1;32m   5278\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pyspark/rdd.py:5251\u001b[0m, in \u001b[0;36m_prepare_for_python_RDD\u001b[0;34m(sc, command)\u001b[0m\n\u001b[1;32m   5248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prepare_for_python_RDD\u001b[39m(sc: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSparkContext\u001b[39m\u001b[38;5;124m\"\u001b[39m, command: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mbytes\u001b[39m, Any, Any, Any]:\n\u001b[1;32m   5249\u001b[0m     \u001b[38;5;66;03m# the serialized command will be compressed by broadcast\u001b[39;00m\n\u001b[1;32m   5250\u001b[0m     ser \u001b[38;5;241m=\u001b[39m CloudPickleSerializer()\n\u001b[0;32m-> 5251\u001b[0m     pickled_command \u001b[38;5;241m=\u001b[39m \u001b[43mser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5252\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   5253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pickled_command) \u001b[38;5;241m>\u001b[39m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mgetBroadcastThreshold(sc\u001b[38;5;241m.\u001b[39m_jsc):  \u001b[38;5;66;03m# Default 1M\u001b[39;00m\n\u001b[1;32m   5254\u001b[0m         \u001b[38;5;66;03m# The broadcast will have same life cycle as created PythonRDD\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pyspark/serializers.py:469\u001b[0m, in \u001b[0;36mCloudPickleSerializer.dumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    467\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not serialize object: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (e\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, emsg)\n\u001b[1;32m    468\u001b[0m print_exec(sys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[0;32m--> 469\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mPicklingError(msg)\n",
      "\u001b[0;31mPicklingError\u001b[0m: Could not serialize object: TypeError: cannot pickle 'weakref.ReferenceType' object"
     ]
    }
   ],
   "source": [
    "epochs=1\n",
    "batch_size=2\n",
    "\n",
    "sc = spark.sparkContext\n",
    "simple_rdd = to_simple_rdd(sc, X_train,y_train)\n",
    "spark_model = SparkModel(model, frequency='epoch', mode='asynchronous',batch_size=batch_size)\n",
    "spark_model.fit(simple_rdd, epochs=epochs, batch_size=batch_size, verbose=0, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52550598-0170-46cd-8c49-70c44204b8d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ee7089-324b-4a40-9483-00aa751430d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caee5c1-ff7c-4148-9ce8-3454be4dbb76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73637eae-64fa-4031-a0ef-cb6a33d9c9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from elephas.spark_model import SparkModel\n",
    "from elephas.utils.rdd_utils import to_simple_rdd\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "# Define basic parameters\n",
    "batch_size = 64\n",
    "nb_classes = 10\n",
    "epochs = 1\n",
    "\n",
    "# Create Spark context\n",
    "# conf = SparkConf().setAppName('Mnist_Spark_MLP').setMaster('local[8]')\n",
    "# sc = SparkContext(conf=conf)\n",
    "\n",
    "# Load data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype(\"float32\")\n",
    "x_test = x_test.astype(\"float32\")\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices\n",
    "y_train = to_categorical(y_train, nb_classes)\n",
    "y_test = to_categorical(y_test, nb_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=784))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.1)\n",
    "model.compile(sgd, 'categorical_crossentropy', ['acc'])\n",
    "\n",
    "# Build RDD from numpy features and labels\n",
    "rdd = to_simple_rdd(sc, x_train, y_train)\n",
    "\n",
    "# Initialize SparkModel from tensorflow.keras model and Spark context\n",
    "spark_model = SparkModel(model, frequency='epoch', mode='asynchronous')\n",
    "\n",
    "# Train Spark model\n",
    "spark_model.fit(rdd, epochs=epochs, batch_size=batch_size, verbose=0, validation_split=0.1)\n",
    "# Evaluate Spark model by evaluating the underlying model\n",
    "score = spark_model.evaluate(x_test, y_test, verbose=2)\n",
    "# print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17db9a2-61ca-47cb-8bc8-9c0122f5ffc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9986a3-a541-41e9-b1ac-d52efd7550c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(simple_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c080e3-af96-4cbb-99a2-51b40266302f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "big_data_project_kernel",
   "language": "python",
   "name": "big_data_project_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

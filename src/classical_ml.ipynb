{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7166ae7b-539f-4adc-9cd7-b73425677bf9",
   "metadata": {},
   "source": [
    "## SVM, Linear Regression, Random Forest, Gradient Boosting,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b644d68c-a9df-4e3a-911c-cff4a4b4ea53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml import Pipeline\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e8b61c0-7e06-42a9-b9b8-acb3023eecb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/14 18:00:38 WARN Utils: Your hostname, alber-victus resolves to a loopback address: 127.0.1.1; using 192.168.1.25 instead (on interface wlp4s0)\n",
      "24/05/14 18:00:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/14 18:00:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/05/14 18:00:39 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"training\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a6648277-ab32-47fa-92d8-5de571710b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_for_ml(stock,train_size=0.8,spark=None,emotion=False,classification=False):\n",
    "\n",
    "\n",
    "\n",
    "    schema = StructType([\n",
    "\n",
    "    StructField('date',StringType(),True),\n",
    "    StructField('afinn_sentiment',FloatType(),True),\n",
    "    StructField('pnn_sentiment',FloatType(),True),\n",
    "    StructField('price_percent_change',FloatType(),True),\n",
    "    StructField('volume_percent_change',FloatType(),True),\n",
    "    StructField('next_day_price_percent_change_shifted',FloatType(),True)\n",
    "    \n",
    "    ])\n",
    "    \n",
    "    # 'date', 'afinn_sentiment', 'pnn_sentiment', 'price_percent_change', 'volume_percent_change', 'next_day_price_percent_change_shifted'\n",
    "    df = spark.read.schema(schema).csv(\"../data/csv/\"+stock+\"/\")\n",
    "\n",
    "    # scale volume\n",
    "\n",
    "        # scale volumne \n",
    "    assembler = VectorAssembler(inputCols=['volume_percent_change'], outputCol=\"features\")\n",
    "\n",
    "    # Transform the data\n",
    "    data = assembler.transform(df)\n",
    "    \n",
    "    # Initialize the StandardScaler\n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_volume_percent_change\", withMean=True, withStd=True)\n",
    "    \n",
    "    # Compute summary statistics by fitting the StandardScaler\n",
    "    scaler_model = scaler.fit(data)\n",
    "    \n",
    "    # Scale features\n",
    "    scaled_data = scaler_model.transform(data)\n",
    "    \n",
    "    firstelement = F.udf(lambda v:float(v[0]),FloatType())\n",
    "    df = scaled_data.withColumn(\"volume_percent_change\", firstelement(\"scaled_volume_percent_change\"))\n",
    "    \n",
    "    df = df.select('date',\n",
    "     'afinn_sentiment',\n",
    "     'pnn_sentiment',\n",
    "     'price_percent_change',\n",
    "     'volume_percent_change',\n",
    "     'next_day_price_percent_change_shifted').withColumnRenamed(\"next_day_price_percent_change_shifted\",\"label\")\n",
    "\n",
    "    \n",
    "    \n",
    "    n = df.count()\n",
    "    train_size = int(n*train_size)\n",
    "\n",
    "    train_data = df.limit(train_size)\n",
    "    test_data = df.subtract(train_data)\n",
    "\n",
    "    if emotion:\n",
    "\n",
    "        \n",
    "        assembler = VectorAssembler(inputCols=['afinn_sentiment', 'pnn_sentiment', 'price_percent_change', 'volume_percent_change'], \\\n",
    "                                    outputCol=\"features\")\n",
    "\n",
    "    else:\n",
    "      \n",
    "        assembler = VectorAssembler(inputCols=['price_percent_change', 'volume_percent_change'], \\\n",
    "                                    outputCol=\"features\")\n",
    "\n",
    "\n",
    "    train_data = Pipeline(stages=[assembler]).fit(train_data).transform(train_data)\n",
    "    test_data = Pipeline(stages=[assembler]).fit(test_data).transform(test_data)\n",
    "\n",
    "    \n",
    "    # X_train = train_data.select(\"features\")\n",
    "    # y_train = train_data.select(\"label\")\n",
    "\n",
    "    # y_train_cat = y_train.withColumn(\"label\", \\\n",
    "    #                                  F.when(F.col(\"label\") >0, 1).otherwise(0))\n",
    "    # X_test = test_data.select(\"features\")\n",
    "    # y_test = test_data.select(\"label\")\n",
    "\n",
    "    # y_test_cat = y_test.withColumn(\"label\", \\\n",
    "    #                                  F.when(F.col(\"label\") >0, 1).otherwise(0))\n",
    "\n",
    "    train_data = train_data.select(\"features\",\"label\")\n",
    "    test_data = test_data.select(\"features\",\"label\")\n",
    "\n",
    "    if classification:\n",
    "        train_data = train_data.withColumn(\"label\", \\\n",
    "                                     F.when(F.col(\"label\") >0, 1).otherwise(0))\n",
    "        \n",
    "        test_data = test_data.withColumn(\"label\", \\\n",
    "                                     F.when(F.col(\"label\") >0, 1).otherwise(0))\n",
    "        \n",
    "    return train_data,test_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3294a6de-89fb-4822-a49d-1fbcd09c8397",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "84934034-df05-48ed-a6ce-00b7636cc133",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import SVMWithSGD, SVMModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.mllib.tree import RandomForest, LabeledPoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f7fdb0a6-090f-4874-9e5f-adebce581ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rdd = train_data.rdd.map(lambda x: LabeledPoint(x[-1],x[:-1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaa236d-e9b0-4db0-9fb1-6627d7be798c",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268254c1-ccd7-49a7-8b55-a18f438af3fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "76067cbb-22fa-4248-9ea5-fc0019ae0f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_lr(stock,emotion,spark):\n",
    "\n",
    "    train_data,test_data = split_data_for_ml(stock,spark=spark,emotion=emotion,classification=False)\n",
    "    \n",
    "    ln_model = LinearRegression(maxIter=1000)\n",
    "    lr_model = ln_model.fit(train_data)\n",
    "    \n",
    "    # Step 6: Evaluate Model\n",
    "    predictions = lr_model.transform(test_data)\n",
    "    evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "    \n",
    "    evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "    r2 = evaluator.evaluate(predictions)\n",
    "\n",
    "    print(stock,str(emotion),rmse,r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "181f4906-deef-4ce9-a508-3e8cc06dfe54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/14 21:05:16 WARN Instrumentation: [bc706089] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple False 1.5498124398532114 -0.020396215477227697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/14 21:05:18 WARN Instrumentation: [e06f8c30] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple True 1.5495442885322075 -0.020043144466917573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/14 21:05:20 WARN Instrumentation: [ee6a1543] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA False 3.254319067465494 -0.03674772912942226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/14 21:05:23 WARN Instrumentation: [12ffd5bf] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA True 3.254015843086086 -0.0365545381518233\n"
     ]
    }
   ],
   "source": [
    "for stock in ['Apple','NVIDIA']:\n",
    "\n",
    "\n",
    "    for emotion in [False,True]:\n",
    "\n",
    "        # model_path = \"../models/\" + \"_binary_\"+ stock + \"_emotion_\" + str(emotion) + \".h5\"\n",
    "        # results_path = \"../results/\" +\"_binary_\"+ stock + \"_emotion_\" + str(emotion) + \".json\"\n",
    "\n",
    "        # key = stock + \"_emotion_\" + str(emotion)\n",
    "\n",
    "        fit_lr(stock,emotion,spark)\n",
    "        \n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e7c42c-1cf0-4c6e-8306-264843fa5b6b",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "679b8f6a-7d78-4573-b9ad-f2659c4e0d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d52ae2df-6f78-4077-a164-c4495c8257bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data = split_data_for_ml(\"Apple\",spark=spark,emotion=True,classification=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "6e703087-82ac-4762-86a8-e3ade1f540d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_svm(stock,emotion,spark):\n",
    "\n",
    "    train_data,test_data = split_data_for_ml(stock,spark=spark,emotion=emotion,classification=True)\n",
    "    \n",
    "\n",
    "    svm = LinearSVC(featuresCol=\"features\", labelCol=\"label\",maxIter=200)\n",
    "    svm_model = svm.fit(train_data)\n",
    "    \n",
    "    # Step 6: Evaluate Model\n",
    "    predictions = svm_model.transform(test_data)\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "    \n",
    "    print(f\"{stock},{str(emotion)}, Accuracy on test data:, {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "c51a53f4-6e0d-4fc2-9d70-c39150706ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/14 21:05:28 ERROR OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n",
      "24/05/14 21:05:29 ERROR OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple,False, Accuracy on test data:, 0.5109289617486339\n",
      "Apple,True, Accuracy on test data:, 0.5109289617486339\n",
      "NVIDIA,False, Accuracy on test data:, 0.574585635359116\n",
      "NVIDIA,True, Accuracy on test data:, 0.574585635359116\n"
     ]
    }
   ],
   "source": [
    "for stock in ['Apple','NVIDIA']:\n",
    "\n",
    "\n",
    "    for emotion in [False,True]:\n",
    "\n",
    "        # model_path = \"../models/\" + \"_binary_\"+ stock + \"_emotion_\" + str(emotion) + \".h5\"\n",
    "        # results_path = \"../results/\" +\"_binary_\"+ stock + \"_emotion_\" + str(emotion) + \".json\"\n",
    "\n",
    "        # key = stock + \"_emotion_\" + str(emotion)\n",
    "\n",
    "        fit_svm(stock,emotion,spark)\n",
    "        \n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a0b2c0-1a8e-47c9-9329-28561e4e9af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "0,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "1d009585-da03-4417-8c59-6a51d4cb65cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "def fit_forest(stock,emotion,spark):\n",
    "\n",
    "    train_data,test_data = split_data_for_ml(stock,spark=spark,emotion=emotion,classification=True)\n",
    "    \n",
    "    # Step 5: Train Random Forest Model\n",
    "    rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\")\n",
    "    rf_model = rf.fit(train_data)\n",
    "    \n",
    "    # Step 6: Evaluate Model\n",
    "    predictions = rf_model.transform(test_data)\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "    \n",
    "    print(f\"{stock},{str(emotion)}, Accuracy on test data:, {accuracy}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a7aed3b2-9e8a-4e30-a39a-068bd2961354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple,False, Accuracy on test data:, 0.5136612021857924\n",
      "Apple,True, Accuracy on test data:, 0.5218579234972678\n",
      "NVIDIA,False, Accuracy on test data:, 0.5386740331491713\n",
      "NVIDIA,True, Accuracy on test data:, 0.569060773480663\n"
     ]
    }
   ],
   "source": [
    "for stock in ['Apple','NVIDIA']:\n",
    "\n",
    "\n",
    "    for emotion in [False,True]:\n",
    "\n",
    "        # model_path = \"../models/\" + \"_binary_\"+ stock + \"_emotion_\" + str(emotion) + \".h5\"\n",
    "        # results_path = \"../results/\" +\"_binary_\"+ stock + \"_emotion_\" + str(emotion) + \".json\"\n",
    "\n",
    "        # key = stock + \"_emotion_\" + str(emotion)\n",
    "\n",
    "        fit_forest(stock,emotion,spark)\n",
    "        \n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34b287a-6d97-4c19-b829-89637692d3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "# Prepare training data\n",
    "data = [(0.0, 0.0, 0.0), (1.0, 1.0, 1.0), (1.0, 2.0, 0.5), (0.0, 0.5, 2.0)]\n",
    "df = spark.createDataFrame(data, [\"label\", \"feature1\", \"feature2\"])\n",
    "\n",
    "# Assemble features into a vector\n",
    "assembler = VectorAssembler(inputCols=[\"feature1\", \"feature2\"], outputCol=\"features\")\n",
    "data = assembler.transform(df)\n",
    "\n",
    "# Split data into training and test sets\n",
    "train_data, test_data = data.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# Train the Gradient Boosted Trees model\n",
    "gbt = GBTClassifier(maxDepth=3, maxBins=32)\n",
    "model = gbt.fit(train_data)\n",
    "\n",
    "# Make predictions and evaluate the model\n",
    "predictions = model.transform(test_data)\n",
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(predictions.count())\n",
    "print(\"Accuracy = \" + str(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "big_data_project_kernel",
   "language": "python",
   "name": "big_data_project_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7166ae7b-539f-4adc-9cd7-b73425677bf9",
   "metadata": {},
   "source": [
    "## SVM, Linear Regression, Random Forest, Gradient Boosting,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b644d68c-a9df-4e3a-911c-cff4a4b4ea53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml import Pipeline\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e8b61c0-7e06-42a9-b9b8-acb3023eecb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/15 15:57:00 WARN Utils: Your hostname, alber-victus resolves to a loopback address: 127.0.1.1; using 10.251.208.55 instead (on interface wlp4s0)\n",
      "24/05/15 15:57:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/15 15:57:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/05/15 15:57:01 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"training\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6648277-ab32-47fa-92d8-5de571710b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_for_ml(stock=None,df=None,train_size=0.8,spark=None,emotion=False,classification=False):\n",
    "\n",
    "\n",
    "\n",
    "    if df is None:\n",
    "        \n",
    "        schema = StructType([\n",
    "    \n",
    "        StructField('date',StringType(),True),\n",
    "        StructField('afinn_sentiment',FloatType(),True),\n",
    "        StructField('pnn_sentiment',FloatType(),True),\n",
    "        StructField('price_percent_change',FloatType(),True),\n",
    "        StructField('volume_percent_change',FloatType(),True),\n",
    "        StructField('next_day_price_percent_change_shifted',FloatType(),True)\n",
    "        \n",
    "        ])\n",
    "        \n",
    "        # 'date', 'afinn_sentiment', 'pnn_sentiment', 'price_percent_change', 'volume_percent_change', 'next_day_price_percent_change_shifted'\n",
    "        df = spark.read.schema(schema).csv(\"../data/csv/\"+stock+\"/\")\n",
    "    \n",
    "        # scale volume\n",
    "\n",
    "        # scale volumne \n",
    "\n",
    "    df = df.sort(df[\"date\"])\n",
    "    assembler = VectorAssembler(inputCols=['volume_percent_change'], outputCol=\"features\")\n",
    "\n",
    "    # Transform the data\n",
    "    data = assembler.transform(df)\n",
    "    \n",
    "    # Initialize the StandardScaler\n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_volume_percent_change\", withMean=True, withStd=True)\n",
    "    \n",
    "    # Compute summary statistics by fitting the StandardScaler\n",
    "    scaler_model = scaler.fit(data)\n",
    "    \n",
    "    # Scale features\n",
    "    scaled_data = scaler_model.transform(data)\n",
    "    \n",
    "    firstelement = F.udf(lambda v:float(v[0]),FloatType())\n",
    "    df = scaled_data.withColumn(\"volume_percent_change\", firstelement(\"scaled_volume_percent_change\"))\n",
    "    \n",
    "    df = df.select('date',\n",
    "     'afinn_sentiment',\n",
    "     'pnn_sentiment',\n",
    "     'price_percent_change',\n",
    "     'volume_percent_change',\n",
    "     'next_day_price_percent_change_shifted').withColumnRenamed(\"next_day_price_percent_change_shifted\",\"label\")\n",
    "\n",
    "\n",
    "    # return df\n",
    "    \n",
    "    n = df.count()\n",
    "    train_size = int(n*train_size)\n",
    "\n",
    "    train_data = df.limit(train_size)\n",
    "    test_data = df.subtract(train_data)\n",
    "\n",
    "    if emotion:\n",
    "\n",
    "        \n",
    "        assembler = VectorAssembler(inputCols=['afinn_sentiment', 'pnn_sentiment', 'price_percent_change', 'volume_percent_change'], \\\n",
    "                                    outputCol=\"features\")\n",
    "\n",
    "    else:\n",
    "      \n",
    "        assembler = VectorAssembler(inputCols=['price_percent_change', 'volume_percent_change'], \\\n",
    "                                    outputCol=\"features\")\n",
    "\n",
    "\n",
    "    train_data = Pipeline(stages=[assembler]).fit(train_data).transform(train_data)\n",
    "    test_data = Pipeline(stages=[assembler]).fit(test_data).transform(test_data)\n",
    "\n",
    "    \n",
    "    # X_train = train_data.select(\"features\")\n",
    "    # y_train = train_data.select(\"label\")\n",
    "\n",
    "    # y_train_cat = y_train.withColumn(\"label\", \\\n",
    "    #                                  F.when(F.col(\"label\") >0, 1).otherwise(0))\n",
    "    # X_test = test_data.select(\"features\")\n",
    "    # y_test = test_data.select(\"label\")\n",
    "\n",
    "    # y_test_cat = y_test.withColumn(\"label\", \\\n",
    "    #                                  F.when(F.col(\"label\") >0, 1).otherwise(0))\n",
    "\n",
    "    train_data = train_data.select(\"features\",\"label\")\n",
    "    test_data = test_data.select(\"features\",\"label\")\n",
    "\n",
    "    if classification:\n",
    "        train_data = train_data.withColumn(\"label\", \\\n",
    "                                     F.when(F.col(\"label\") >0, 1).otherwise(0))\n",
    "        \n",
    "        test_data = test_data.withColumn(\"label\", \\\n",
    "                                     F.when(F.col(\"label\") >0, 1).otherwise(0))\n",
    "        \n",
    "    return train_data,test_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84934034-df05-48ed-a6ce-00b7636cc133",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import SVMWithSGD, SVMModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.mllib.tree import RandomForest, LabeledPoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7fdb0a6-090f-4874-9e5f-adebce581ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_rdd = train_data.rdd.map(lambda x: Labeled<Point(x[-1],x[:-1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaa236d-e9b0-4db0-9fb1-6627d7be798c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268254c1-ccd7-49a7-8b55-a18f438af3fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76067cbb-22fa-4248-9ea5-fc0019ae0f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_lr(stock,emotion,spark):\n",
    "\n",
    "    train_data,test_data = split_data_for_ml(stock,spark=spark,emotion=emotion,classification=False)\n",
    "    \n",
    "    ln_model = LinearRegression(maxIter=1000)\n",
    "    lr_model = ln_model.fit(train_data)\n",
    "    \n",
    "    # Step 6: Evaluate Model\n",
    "    predictions = lr_model.transform(test_data)\n",
    "    evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "    \n",
    "    evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "    r2 = evaluator.evaluate(predictions)\n",
    "\n",
    "    print(stock,str(emotion),rmse,r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "181f4906-deef-4ce9-a508-3e8cc06dfe54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/15 13:48:05 WARN Instrumentation: [92284651] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/05/15 13:48:06 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple False 1.5498124398532114 -0.020396215477227697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/15 13:48:10 WARN Instrumentation: [02b8aaa8] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple True 1.5495442885322075 -0.020043144466917573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/15 13:48:13 WARN Instrumentation: [e727771c] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA False 3.254319067465494 -0.03674772912942226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/15 13:48:16 WARN Instrumentation: [38e394cc] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA True 3.254015843086086 -0.0365545381518233\n"
     ]
    }
   ],
   "source": [
    "for stock in ['Apple','NVIDIA']:\n",
    "\n",
    "\n",
    "    for emotion in [False,True]:\n",
    "\n",
    "        # model_path = \"../models/\" + \"_binary_\"+ stock + \"_emotion_\" + str(emotion) + \".h5\"\n",
    "        # results_path = \"../results/\" +\"_binary_\"+ stock + \"_emotion_\" + str(emotion) + \".json\"\n",
    "\n",
    "        # key = stock + \"_emotion_\" + str(emotion)\n",
    "\n",
    "        fit_lr(stock,emotion,spark)\n",
    "        \n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e7c42c-1cf0-4c6e-8306-264843fa5b6b",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "679b8f6a-7d78-4573-b9ad-f2659c4e0d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LinearSVC,LinearSVCModel\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "973b4474-15e9-47ab-8aa0-da4dc0890ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = []\n",
    "emotions = []\n",
    "accuracies = []\n",
    "models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d52ae2df-6f78-4077-a164-c4495c8257bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data = split_data_for_ml(\"Apple\",spark=spark,emotion=True,classification=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e703087-82ac-4762-86a8-e3ade1f540d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_svm(stock,emotion,spark):\n",
    "\n",
    "    train_data,test_data = split_data_for_ml(stock,spark=spark,emotion=emotion,classification=True)\n",
    "\n",
    "\n",
    "    svm = LinearSVC(featuresCol=\"features\", labelCol=\"label\",maxIter=200)\n",
    "    svm_model = svm.fit(train_data)\n",
    "    \n",
    "    # Step 6: Evaluate Model\n",
    "    predictions = svm_model.transform(test_data)\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "    \n",
    "    print(f\"{stock},{str(emotion)}, Accuracy on test data:, {accuracy}\")\n",
    "\n",
    "    return accuracy,svm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c51a53f4-6e0d-4fc2-9d70-c39150706ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/15 16:45:03 ERROR OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n",
      "24/05/15 16:45:03 ERROR OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple,False, Accuracy on test data:, 0.5109289617486339\n",
      "Apple,True, Accuracy on test data:, 0.5109289617486339\n",
      "NVIDIA,False, Accuracy on test data:, 0.574585635359116\n",
      "NVIDIA,True, Accuracy on test data:, 0.574585635359116\n"
     ]
    }
   ],
   "source": [
    "for stock in ['Apple','NVIDIA']:\n",
    "\n",
    "\n",
    "    for emotion in [False,True]:\n",
    "\n",
    "        # model_path = \"../models/\" + \"_binary_\"+ stock + \"_emotion_\" + str(emotion) + \".h5\"\n",
    "        # results_path = \"../results/\" +\"_binary_\"+ stock + \"_emotion_\" + str(emotion) + \".json\"\n",
    "\n",
    "        # key = stock + \"_emotion_\" + str(emotion)\n",
    "\n",
    "        accuracy,model = fit_svm(stock,emotion,spark)\n",
    "\n",
    "        # model_path = \"../models/\" + \"_binary_smv\"+stock+ \"_emotion_\" + str(emotion)\n",
    "        # model.write().overwrite().save(model_path)\n",
    "\n",
    "        models.append(\"SVM\")\n",
    "        stocks.append(stock)\n",
    "        emotions.append(str(emotion))\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d36234-73c2-4df1-bec7-f23f56649207",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1d009585-da03-4417-8c59-6a51d4cb65cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "def fit_forest(stock,emotion,spark):\n",
    "\n",
    "    train_data,test_data = split_data_for_ml(stock,spark=spark,emotion=emotion,classification=True)\n",
    "    \n",
    "    # Step 5: Train Random Forest Model\n",
    "    rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\")\n",
    "    rf_model = rf.fit(train_data)\n",
    "    \n",
    "    # Step 6: Evaluate Model\n",
    "    predictions = rf_model.transform(test_data)\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "    \n",
    "    print(f\"{stock},{str(emotion)}, Accuracy on test data:, {accuracy}\")\n",
    "\n",
    "    return accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a7aed3b2-9e8a-4e30-a39a-068bd2961354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple,False, Accuracy on test data:, 0.5245901639344263\n",
      "Apple,True, Accuracy on test data:, 0.5081967213114754\n",
      "NVIDIA,False, Accuracy on test data:, 0.505524861878453\n",
      "NVIDIA,True, Accuracy on test data:, 0.569060773480663\n"
     ]
    }
   ],
   "source": [
    "for stock in ['Apple','NVIDIA']:\n",
    "\n",
    "\n",
    "    for emotion in [False,True]:\n",
    "\n",
    "        # model_path = \"../models/\" + \"_binary_\"+ stock + \"_emotion_\" + str(emotion) + \".h5\"\n",
    "        # results_path = \"../results/\" +\"_binary_\"+ stock + \"_emotion_\" + str(emotion) + \".json\"\n",
    "\n",
    "        # key = stock + \"_emotion_\" + str(emotion)\n",
    "\n",
    "        accuracy = fit_forest(stock,emotion,spark)\n",
    "\n",
    "        models.append(\"RandomForest\")\n",
    "        stocks.append(stock)\n",
    "        emotions.append(str(emotion))\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a34b287a-6d97-4c19-b829-89637692d3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "def fit_gb(stock,emotion,spark):\n",
    "\n",
    "    train_data,test_data = split_data_for_ml(stock,spark=spark,emotion=emotion,classification=True)\n",
    "    \n",
    "    gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\", maxIter=100)\n",
    "    \n",
    "    # Train the model\n",
    "    gbt_model = gbt.fit(train_data)\n",
    "    \n",
    "    # Make predictions on the test data\n",
    "    predictions = gbt_model.transform(test_data)\n",
    "    \n",
    "    # Evaluate model\n",
    "    evaluator = BinaryClassificationEvaluator(labelCol=\"label\")\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "    print(f\"{stock},{str(emotion)}, Accuracy on test data:, {accuracy}\")\n",
    "    \n",
    "    # Stop SparkSession\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9ce82c4d-06a9-457b-921a-e972cd05c159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple,False, Accuracy on test data:, 0.5041974128401995\n",
      "Apple,True, Accuracy on test data:, 0.523586173931228\n",
      "NVIDIA,False, Accuracy on test data:, 0.5159996253746254\n",
      "NVIDIA,True, Accuracy on test data:, 0.5127684815184815\n"
     ]
    }
   ],
   "source": [
    "for stock in ['Apple','NVIDIA']:\n",
    "\n",
    "\n",
    "    for emotion in [False,True]:\n",
    "\n",
    "        # model_path = \"../models/\" + \"_binary_\"+ stock + \"_emotion_\" + str(emotion) + \".h5\"\n",
    "        # results_path = \"../results/\" +\"_binary_\"+ stock + \"_emotion_\" + str(emotion) + \".json\"\n",
    "\n",
    "        # key = stock + \"_emotion_\" + str(emotion)\n",
    "\n",
    "        accuracy = fit_gb(stock,emotion,spark)\n",
    "\n",
    "        \n",
    "        models.append(\"GradientBoosting\")\n",
    "        stocks.append(stock)\n",
    "        emotions.append(str(emotion))\n",
    "        accuracies.append(accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fa92e7-35ef-41cc-aa12-c07bb0a56efd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b531ac9b-f8a3-4b6f-b946-c98c57bb7b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5109289617486339,\n",
       " 0.5109289617486339,\n",
       " 0.574585635359116,\n",
       " 0.574585635359116,\n",
       " 0.5245901639344263,\n",
       " 0.5081967213114754,\n",
       " 0.505524861878453,\n",
       " 0.569060773480663,\n",
       " 0.5041974128401995,\n",
       " 0.523586173931228,\n",
       " 0.5159996253746254,\n",
       " 0.5127684815184815]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b50e1aae-e565-463e-8917-5add1387f355",
   "metadata": {},
   "outputs": [],
   "source": [
    "lists = [(models[i],stocks[i],emotions[i],accuracies[i]) for i in range(len(accuracies))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e8aafb87-305f-4757-a2d3-cfcb69119327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SVM', 'Apple', 'False', 0.5109289617486339),\n",
       " ('SVM', 'Apple', 'True', 0.5109289617486339),\n",
       " ('SVM', 'NVIDIA', 'False', 0.574585635359116),\n",
       " ('SVM', 'NVIDIA', 'True', 0.574585635359116),\n",
       " ('RandomForest', 'Apple', 'False', 0.5245901639344263),\n",
       " ('RandomForest', 'Apple', 'True', 0.5081967213114754),\n",
       " ('RandomForest', 'NVIDIA', 'False', 0.505524861878453),\n",
       " ('RandomForest', 'NVIDIA', 'True', 0.569060773480663),\n",
       " ('GradientBoosting', 'Apple', 'False', 0.5041974128401995),\n",
       " ('GradientBoosting', 'Apple', 'True', 0.523586173931228),\n",
       " ('GradientBoosting', 'NVIDIA', 'False', 0.5159996253746254),\n",
       " ('GradientBoosting', 'NVIDIA', 'True', 0.5127684815184815)]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5bf3e760-a40e-408b-b188-c4fa3c4fb468",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "\n",
    "    StructField('model',StringType(),True),\n",
    "    StructField('stock',StringType(),True),\n",
    "    StructField('emotion',StringType(),True),\n",
    "    StructField('accuracy',FloatType(),True),\n",
    "    \n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "430c584f-2c61-40f6-8d3a-a57ebb5a6b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(lists,schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "00194e10-09d5-41e6-91f4-97ce676e74a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+-------+----------+\n",
      "|           model| stock|emotion|  accuracy|\n",
      "+----------------+------+-------+----------+\n",
      "|             SVM| Apple|  False|  0.510929|\n",
      "|             SVM| Apple|   True|  0.510929|\n",
      "|             SVM|NVIDIA|  False| 0.5745856|\n",
      "|             SVM|NVIDIA|   True| 0.5745856|\n",
      "|    RandomForest| Apple|  False|0.52459013|\n",
      "|    RandomForest| Apple|   True| 0.5081967|\n",
      "|    RandomForest|NVIDIA|  False| 0.5055249|\n",
      "|    RandomForest|NVIDIA|   True| 0.5690608|\n",
      "|GradientBoosting| Apple|  False| 0.5041974|\n",
      "|GradientBoosting| Apple|   True|0.52358615|\n",
      "|GradientBoosting|NVIDIA|  False| 0.5159996|\n",
      "|GradientBoosting|NVIDIA|   True| 0.5127685|\n",
      "+----------------+------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c000b306-04b1-4abb-8443-862256176879",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.toPandas().to_csv(\"../results/ml_classificatons.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3c721a50-e14a-4b82-b2d2-fce9d3fba55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[0.86353588104248...|    1|\n",
      "|[1.28221988677978...|    1|\n",
      "|[0.97755247354507...|    1|\n",
      "|[1.20787966251373...|    1|\n",
      "|[1.33455538749694...|    1|\n",
      "|[1.17594683170318...|    0|\n",
      "|[0.72665840387344...|    0|\n",
      "|[1.90191984176635...|    1|\n",
      "|[1.43675780296325...|    0|\n",
      "|[1.26209092140197...|    0|\n",
      "|[1.17231535911560...|    1|\n",
      "|[1.74015557765960...|    1|\n",
      "|[0.88406747579574...|    0|\n",
      "|[0.68886607885360...|    1|\n",
      "|[1.45449090003967...|    1|\n",
      "|[0.55309116840362...|    1|\n",
      "|[1.34638464450836...|    0|\n",
      "|[0.48672020435333...|    0|\n",
      "|[1.31998777389526...|    1|\n",
      "|[1.43784916400909...|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "72e51166-58d5-4f54-b0ba-467a42e8d73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "    schema = StructType([\n",
    "\n",
    "    StructField('date',StringType(),True),\n",
    "    StructField('afinn_sentiment',FloatType(),True),\n",
    "    StructField('pnn_sentiment',FloatType(),True),\n",
    "    StructField('price_percent_change',FloatType(),True),\n",
    "    StructField('volume_percent_change',FloatType(),True),\n",
    "    StructField('next_day_price_percent_change_shifted',FloatType(),True)\n",
    "    \n",
    "    ])\n",
    "    \n",
    "    # 'date', 'afinn_sentiment', 'pnn_sentiment', 'price_percent_change', 'volume_percent_change', 'next_day_price_percent_change_shifted'\n",
    "    df = spark.read.schema(schema).csv(\"../data/csv/\"+stock+\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "37623d2a-4ea0-4617-bec1-8614d67b4d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed(\"next_day_price_percent_change_shifted\",\"next_day_price_percent_change\")\n",
    "df = df.withColumn(\"label\",F.col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1602d352-1816-4b8d-8d84-e2ff320fa901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+-------------+--------------------+---------------------+-----------------------------+\n",
      "|      date|afinn_sentiment|pnn_sentiment|price_percent_change|volume_percent_change|next_day_price_percent_change|\n",
      "+----------+---------------+-------------+--------------------+---------------------+-----------------------------+\n",
      "|2017-01-04|      0.9344968|          1.0|           2.3331146|           -20.158243|                   -2.5385644|\n",
      "|2017-01-05|      1.1022971|    0.9130435|          -2.5385644|           -17.921982|                    1.3367225|\n",
      "|2017-01-06|      0.8313522|          1.0|           1.3367225|            -16.40157|                     4.054336|\n",
      "|2017-01-09|      1.1039267|          1.0|            4.054336|            11.349738|                  -0.75503397|\n",
      "|2017-01-10|     0.95401067|          0.6|         -0.75503397|           -3.8557246|                   -1.2303843|\n",
      "|2017-01-11|      1.0971123|          1.0|          -1.2303843|            -40.32784|                   -1.6356016|\n",
      "|2017-01-12|      0.5437861|          1.0|          -1.6356016|            19.014427|                 -0.009698672|\n",
      "|2017-01-13|     -0.1431016|          1.0|        -0.009698672|           -26.820925|                    -2.243028|\n",
      "|2017-01-17|      0.6773476|          1.0|           -2.243028|            26.821022|                    1.8197842|\n",
      "|2017-01-18|      1.9020587|         0.75|           1.8197842|            12.712792|                    2.1466835|\n",
      "|2017-01-19|      1.0335115|          1.0|           2.1466835|           -1.4596041|                   -1.0935934|\n",
      "|2017-01-20|      2.3532264|          1.0|          -1.0935934|           -23.702068|                    1.0383525|\n",
      "|2017-01-23|      1.8698609|          0.2|           1.0383525|           -24.819927|                    2.1315207|\n",
      "|2017-01-24|      2.0225027|          1.0|           2.1315207|            15.719754|                    0.4285991|\n",
      "|2017-01-25|      3.8828235|          1.0|           0.4285991|            22.125349|                    1.7255579|\n",
      "|2017-01-26|      1.7172192|          1.0|           1.7255579|            -12.23506|                    1.9334234|\n",
      "|2017-01-27|      0.7632085|    0.5555556|           1.9334234|            2.8761988|                   -1.5657182|\n",
      "|2017-01-30|      1.5836577|          1.0|          -1.5657182|             5.086484|                  -0.76348066|\n",
      "|2017-01-31|      2.4433274|          1.0|         -0.76348066|             -26.8643|                    4.3689165|\n",
      "|2017-02-02|     0.73935825|          1.0|           1.2637191|            -18.09547|                   -0.8752961|\n",
      "+----------+---------------+-------------+--------------------+---------------------+-----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d95285-c93b-4d64-93f5-97c6d93d4433",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "big_data_project_kernel",
   "language": "python",
   "name": "big_data_project_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
